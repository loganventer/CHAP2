#!/bin/bash

echo "üöÄ Restarting comprehensive deployment with GPU fixes..."

# Stop existing containers
echo "üì¶ Stopping existing containers..."
docker-compose -f docker-compose.gpu.yml down

# Remove any existing Ollama models to force re-download with GPU
echo "üóëÔ∏è  Removing existing Ollama models to force GPU re-download..."
docker volume rm langchain_search_service_ollama_models 2>/dev/null || echo "No existing models to remove"

# Start with GPU support
echo "üîß Starting containers with comprehensive GPU support..."
docker-compose -f docker-compose.gpu.yml up -d

# Wait for services to be ready
echo "‚è≥ Waiting for services to be ready..."
sleep 30

# Pull models with GPU support
echo "üì¶ Pulling models with GPU support..."
docker exec -it langchain_search_service-ollama-1 ollama pull mistral
docker exec -it langchain_search_service-ollama-1 ollama pull nomic-embed-text

# Verify GPU usage
echo "üîç Verifying GPU usage..."
docker exec -it langchain_search_service-ollama-1 nvidia-smi 2>/dev/null || echo "‚ö†Ô∏è  GPU not detected"

# Test GPU inference
echo "üß™ Testing GPU inference..."
docker exec -it langchain_search_service-ollama-1 ollama run mistral "Test GPU inference" 2>/dev/null || echo "‚ö†Ô∏è  Could not test GPU inference"

echo "‚úÖ Comprehensive GPU deployment complete!"
echo ""
echo "üåê Service URLs:"
echo "- Web Portal: http://localhost:5000"
echo "- API: http://localhost:5001"
echo "- LangChain: http://localhost:8000"
echo "- Qdrant: http://localhost:6333"
echo "- Ollama: http://localhost:11434"
echo ""
echo "üîç To monitor GPU usage:"
echo "   docker exec -it langchain_search_service-ollama-1 nvidia-smi"
echo ""
echo "üìä To check if GPU is being used during AI search:"
echo "   docker exec -it langchain_search_service-ollama-1 nvidia-smi -l 1" 